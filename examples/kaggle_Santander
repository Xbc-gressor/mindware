import pandas as pd
import os
import sys

# 将当前文件所在文件夹的上层目录加入到sys.path中
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from mindware.utils.data_manager import DataManager
from mindware import CASHFE
from mindware import CASH
from mindware import HPO
from mindware import EnsembleBuilder
from mindware import CLASSIFICATION
import pickle as pkl
from sklearn.preprocessing import normalize
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectPercentile
from sklearn.feature_selection import f_classif,chi2
from sklearn.preprocessing import Binarizer, scale
import argparse
import time
from datetime import datetime

if __name__ == '__main__':
    start_time = time.time()
    start_datetime = datetime.fromtimestamp(start_time)

    # 格式化为日期字符串
    formatted_start_time = start_datetime.strftime('%Y-%m-%d %H:%M:%S')
    # 从命令行参数中解析出参数
    parser = argparse.ArgumentParser()
    parser.add_argument('--Opt', type=str, default='cashfe', help='cash or cashfe')
    parser.add_argument('--optimizer', type=str, default='smac', help='smac or mab')
    parser.add_argument('--x_encode', type=str, default=None, help='smac or mab')
    parser.add_argument('--ensemble_method', type=str, default='blending', help='ensemble_selection or blending')
    parser.add_argument('--ensemble_size', type=int, default=10, help='ensemble size')
    parser.add_argument('--evaluation', type=str, default='holdout', help='evaluation')
    parser.add_argument('--time_limit', type=int, default=14400, help='time limit') # 5024
    parser.add_argument('--per_time_limit', type=int, default=300, help='time limit') # 600
    args = parser.parse_args()

    Opt = args.Opt

    task_type = CLASSIFICATION

    optimizer = args.optimizer
    x_encode = args.x_encode
    ensemble_method = args.ensemble_method
    ensemble_size = args.ensemble_size
    metric = 'auc'
    evaluation = 'holdout'
    time_limit = args.time_limit
    per_time_limit = args.per_time_limit
    estimator_id = 'neural_network'

    # Load data
    data_dir = r"/home/liuwei/mindware_data/Santander Customer Satisfaction"
    # data_dir = '/root/automl_data/kaggle/santander'

    dm = DataManager()

    _train_data_node = dm.load_train_csv(os.path.join(data_dir, 'train.csv'), ignore_columns=['ID'],
                                         label_name='TARGET')
    train_data_node = dm.preprocess_fit(_train_data_node, task_type, x_encode=x_encode)

    test_data_node = dm.load_test_csv(os.path.join(data_dir, 'test.csv'))
    test_data_node = dm.preprocess_transform(test_data_node)
    
    # Initialize CASHFE
    include_algorithms = [
        'xgboost','lasso_regression', 'liblinear_svr', 'libsvm_svr',
             'ridge_regression','neural_network'
    ]
    '''
            'lasso_regression', 'liblinear_svr', 'libsvm_svr',
            'random_forest', 'ridge_regression'lightgbm
    '''
    if Opt == 'cash':
        # 'lda',
        OPT = CASH
    else:
        OPT = CASHFE

    x_encode_str = '' if x_encode is None else ('_' + x_encode)
    passenger_id = pd.read_csv(os.path.join(data_dir, 'test.csv'))['ID']

    opt_hpo = CASHFE(
        include_algorithms=include_algorithms, sub_optimizer='smac', task_type=CLASSIFICATION,
        metric=metric,
        data_node=train_data_node, evaluation=evaluation, resampling_params=None,
        optimizer='mab', inner_iter_num_per_iter=5,
        time_limit=time_limit, amount_of_resource=100000, per_run_time_limit=per_time_limit,
        output_dir='./data', seed=1, n_jobs=1,
        ensemble_method=ensemble_method, ensemble_size=5
    ) # 原seed为1
    print(opt_hpo.run())
    pred_hpo = opt_hpo.predict(test_data_node, ens=False, prob=True)[:,1]
    pred_hpo_with_ens = opt_hpo.predict(test_data_node, ens=True, prob=True)[:,1]
    # pred_hpo = dm.decode_label(pred_hpo)
    result_hpo = pd.DataFrame({'ID': passenger_id, 'TARGET': pred_hpo})
    result_hpo_with_ens = pd.DataFrame({'ID': passenger_id, 'TARGET': pred_hpo_with_ens})
    result_hpo.to_csv(os.path.join(data_dir,
                               f's102{estimator_id}{x_encode_str}_{evaluation}_{optimizer}{time_limit}_{ensemble_method}{ensemble_size}_{formatted_start_time}_result.csv'),
                  index=False)
    result_hpo_with_ens.to_csv(os.path.join(data_dir,
                               f's102{estimator_id}{x_encode_str}_{evaluation}_{optimizer}{time_limit}_{ensemble_method}{ensemble_size}_{formatted_start_time}_result_with_ens.csv'),
                  index=False)
    print('Result has been saved to result_hpo.csv.')

    # import pickle as pkl
    # topk = pkl.load(open('/home/liuwei/mindware/examples/data/CASHFE-mab(1)-holdout_2024-12-18-16-34-21-940759/2024-12-18-16-34-21-940759_topk_config.pkl', 'rb'))

    # opt_hpo = CASHFE(
    #     include_algorithms=None, sub_optimizer='smac', task_type=CLASSIFICATION,
    #     metric=metric,
    #     data_node=train_data_node, evaluation=evaluation, resampling_params=None,
    #     optimizer='smac', inner_iter_num_per_iter=5,
    #     time_limit=time_limit, amount_of_resource=100000, per_run_time_limit=per_time_limit,
    #     output_dir='./data', seed=1, n_jobs=1,
    #     ensemble_method=ensemble_method, ensemble_size=5
    # )

    # opt_hpo._predict_stats(test_data_node, stats=topk, ens=True)
  








    # train_X = train_data_node.data[0]
    # for i in range(len(train_X)):
    #     for j in range(len(train_X[0])):
    #         # 手动处理-99999
    #         if train_X[i][j] == -999999:
    #             train_X[i][j] = 2
    # # 添加0列
    # train_X = pd.DataFrame(train_X)
    # train_X['n0'] = (train_X == 0).sum(axis=1)
    # X_normalized = normalize(train_X, axis=0)
    # pca = PCA(n_components=2)
    # X_pca = pca.fit_transform(X_normalized)
    # train_X['PCA1'] = X_pca[:,0]
    # train_X['PCA2'] = X_pca[:,1]
    # train_y = train_data_node.data[1]
    # p = 75
    # X_bin = Binarizer().fit_transform(scale(train_X))
    # selectChi2 = SelectPercentile(chi2, percentile=p).fit(X_bin, train_y)
    # selectF_classif = SelectPercentile(f_classif, percentile=p).fit(train_X, train_y)

    # chi2_selected = selectChi2.get_support()
    # chi2_selected_features = [ f for i,f in enumerate(train_X.columns) if chi2_selected[i]]
    # f_classif_selected = selectF_classif.get_support()
    # f_classif_selected_features = [ f for i,f in enumerate(train_X.columns) if f_classif_selected[i]]
    # selected = chi2_selected & f_classif_selected
    # features = [f for f,s in zip(train_X.columns, selected) if s]
    # X_sel = train_X[features]
    # X_sel = X_sel.to_numpy()
    # print(len(X_sel[0]))
    # train_data_node.data = (X_sel,train_data_node.data[1])

    # test = test_data_node.data[0]
    # test = pd.DataFrame(test)
    # test['n0'] = (test == 0).sum(axis=1)
    # # test['logvar38'] = test['var38'].map(np.log1p)
    # # # Encode var36 as category
    # # test['var36'] = test['var36'].astype('category')
    # # test = pd.get_dummies(test)
    # test_normalized = normalize(test, axis=0)
    # pca = PCA(n_components=2)
    # test_pca = pca.fit_transform(test_normalized)
    # test['PCA1'] = test_pca[:,0]
    # test['PCA2'] = test_pca[:,1]
    # test = test[features]
    # test = test.to_numpy()
    # test_data_node.data = (test,test_data_node.data[1])